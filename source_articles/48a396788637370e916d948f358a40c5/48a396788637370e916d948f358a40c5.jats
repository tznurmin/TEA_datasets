<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1186/s13007-019-0479-8</query><apiKey>6b19ee21fe6a243741a528650f7c8997</apiKey><result><total>1</total><start>1</start><pageLength>1</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><?properties open_access?><front><journal-meta><journal-id journal-id-type="publisher-id">13007</journal-id><journal-title-group><journal-title>Plant Methods</journal-title><abbrev-journal-title abbrev-type="publisher">Plant Methods</abbrev-journal-title></journal-title-group><issn pub-type="epub">1746-4811</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s13007-019-0479-8</article-id><article-id pub-id-type="manuscript">479</article-id><article-id pub-id-type="doi">10.1186/s13007-019-0479-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Plant disease identification using explainable 3D deep learning on hyperspectral images</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><name><surname>Nagasubramanian</surname><given-names>Koushik</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Jones</surname><given-names>Sarah</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Singh</surname><given-names>Asheesh K.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au4"><name><surname>Sarkar</surname><given-names>Soumik</given-names></name><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="corresp" rid="IDs1300701904798_cor4">d</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au5"><name><surname>Singh</surname><given-names>Arti</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs1300701904798_cor5">e</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au6"><name><surname>Ganapathysubramanian</surname><given-names>Baskar</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="corresp" rid="IDs1300701904798_cor6">f</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7312</institution-id><institution-id institution-id-type="GRID">grid.34421.30</institution-id><institution content-type="org-division">Department of Electrical and Computer Engineering</institution><institution content-type="org-name">Iowa State University</institution></institution-wrap><addr-line content-type="city">Ames</addr-line><addr-line content-type="state">IA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7312</institution-id><institution-id institution-id-type="GRID">grid.34421.30</institution-id><institution content-type="org-division">Department of Agronomy</institution><institution content-type="org-name">Iowa State University</institution></institution-wrap><addr-line content-type="city">Ames</addr-line><addr-line content-type="state">IA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7312</institution-id><institution-id institution-id-type="GRID">grid.34421.30</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Iowa State University</institution></institution-wrap><addr-line content-type="city">Ames</addr-line><addr-line content-type="state">IA</addr-line><country country="US">USA</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7312</institution-id><institution-id institution-id-type="GRID">grid.34421.30</institution-id><institution content-type="org-name">Plant Sciences Institute, Iowa State University</institution></institution-wrap><addr-line content-type="city">Ames</addr-line><addr-line content-type="state">IA</addr-line><country country="US">USA</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7312</institution-id><institution-id institution-id-type="GRID">grid.34421.30</institution-id><institution content-type="org-division">Department of Computer Science</institution><institution content-type="org-name">Iowa State University</institution></institution-wrap><addr-line content-type="city">Ames</addr-line><addr-line content-type="state">IA</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs1300701904798_cor4"><label>d</label><email>soumiks@iastate.edu</email></corresp><corresp id="IDs1300701904798_cor5"><label>e</label><email>arti@iastate.edu</email></corresp><corresp id="IDs1300701904798_cor6"><label>f</label><email>baskarg@iastate.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>21</day><month>8</month><year>2019</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2019</year></pub-date><volume>15</volume><issue seq="98">1</issue><elocation-id>98</elocation-id><history><date date-type="registration"><day>6</day><month>8</month><year>2019</year></date><date date-type="received"><day>26</day><month>3</month><year>2019</year></date><date date-type="accepted"><day>6</day><month>8</month><year>2019</year></date><date date-type="online"><day>21</day><month>8</month><year>2019</year></date></history><permissions><copyright-statement>© The Author(s) 2019</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="url">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link xlink:href="http://creativecommons.org/publicdomain/zero/1.0/" ext-link-type="url">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1" xml:lang="en"><title>Abstract</title><sec id="ASec1"><title>Background</title><p id="Par1">Hyperspectral imaging is emerging as a promising approach for plant disease identification. The large and possibly redundant information contained in hyperspectral data cubes makes deep learning based identification of plant diseases a natural fit. Here, we deploy a novel 3D deep convolutional neural network (DCNN) that directly assimilates the hyperspectral data. Furthermore, we interrogate the learnt model to produce physiologically meaningful explanations. We focus on an economically important disease, charcoal rot, which is a soil borne fungal disease that affects the yield of soybean crops worldwide.</p></sec><sec id="ASec2"><title>Results</title><p id="Par2">Based on hyperspectral imaging of inoculated and mock-inoculated stem images, our 3D DCNN has a classification accuracy of 95.73% and an infected class F1 score of 0.87. Using the concept of a saliency map, we visualize the most sensitive pixel locations, and show that the spatial regions with visible disease symptoms are overwhelmingly chosen by the model for classification. We also find that the most sensitive wavelengths used by the model for classification are in the near infrared region (NIR), which is also the commonly used spectral range for determining the vegetative health of a plant.</p></sec><sec id="ASec3"><title>Conclusion</title><p id="Par3">The use of an explainable deep learning model not only provides high accuracy, but also provides physiological insight into model predictions, thus generating confidence in model predictions. These explained predictions lend themselves for eventual use in precision agriculture and research application using automated phenotyping platforms.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Deep convolutional neural network</kwd><kwd>Charcoal rot disease</kwd><kwd>Soybean</kwd><kwd>Saliency map</kwd><kwd>Hyperspectral</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>Iowa Soybean Association</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/100011461</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>National Institute of Food and Agriculture</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/100005825</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">2017-67007-26151</award-id><principal-award-recipient><name><surname>Sarkar</surname><given-names>Soumik</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Iowa State University</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/100009227</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>USDA-CRIS</institution></institution-wrap></funding-source><award-id award-type="FundRef grant">IOW04314</award-id><principal-award-recipient><name><surname>Singh</surname><given-names>Asheesh K.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>BioMed Central</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>8</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/13007_2019_Article_479.pdf</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Life Sciences</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Plant Sciences</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Biological Techniques</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Biomedical and Life Sciences</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta><notes notes-type="AuthorContribution"><p>Koushik Nagasubramanian and Sarah Jones contributed equally to this work</p></notes></front><body><sec id="Sec1"><title>Background</title><p id="Par4">Plant diseases negatively impact yield potential of crops worldwide, including soybean [<italic>Glycine max</italic> (L.) Merr.], reducing the average annual soybean yield by an estimated 11% in the United States [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. From 2010 to 2014, soybean economic damage due to diseases have accounted for over an estimated $23 billion US dollars in the United States and Canada alone making efforts to predict and control disease outbreaks as well as develop disease resistant soybean varieties of economic importance [<xref ref-type="bibr" rid="CR3">3</xref>]. However, today’s disease scouting and phenotyping techniques rely on human scouts and visual ratings. Human visual ratings are dependent on rater ability, rater reliability, and can be prone to human error, subjectivity, and inter/intra-rater variability [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. There is an established need for improved technologies for disease detection and identification beyond visual ratings in order to improve yield protection through mitigation strategies.</p><p id="Par5">Charcoal rot, <italic>Macrophomina phaseolina</italic> (Tassi) Goid, is an important fungal disease for producers in the United States and Canada ranking among the top seven most severe diseases in soybean from 2006 to 2014 and as high as the 2nd most yield limiting soybean disease in 2012 [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]. Charcoal rot has a large host range affecting other important economic crops such as corn, cotton, and sorghum making crop rotation a difficult management strategy [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. In addition, there are limited chemical control measures leaving resistance breeding as an important approach to manage charcoal rot in soybean [<xref ref-type="bibr" rid="CR11">11</xref>]. Symptoms of infection include reddish-brown lesions on the hypocotyl of seedlings, but are generally not seen until the later developmental stages, R5–R7, as a reddish-brown discoloration of the vascular tissue, wilting, chlorosis, and early senesce of plants leaving leaves and petioles still attached to the plant [<xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref>]. Small black fungal survival structures called microsclerotia, which also act as an inoculum source, develop at the nodes and in the epidermal and sub epidermal tissue, pods, and even on seed [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. In an effort to develop methods for earlier screening of resistance, one study proposed the cut-stem inoculation method for inoculating soybean seedlings in a growth chamber or greenhouse environment to measure lesion progression within a month after planting [<xref ref-type="bibr" rid="CR15">15</xref>]. Recently, a Genome Wide Association (GWA) study reported a total of 19 SNPs associated with charcoal rot resistance in soybean [<xref ref-type="bibr" rid="CR16">16</xref>]. However, both field scouting for disease detection and small-scale methods for charcoal rot evaluation still rely on visual ratings. These field and greenhouse screening methods for charcoal rot are time consuming and labor intensive.</p><p id="Par6">Unlike visual ratings, which only utilize visible wavelengths, hyperspectral imaging can capture spectral and spatial information from wavelengths beyond human vision, offering more usable information for disease detection. In addition, hyperspectral imaging offers a potential solution to the scalability and repeatability issues faced with human visual ratings. In [<xref ref-type="bibr" rid="CR17">17</xref>], the authors investigated hyperspectral image analysis techniques for early detection and classification of plant diseases. Hyperspectral imaging has been used for the detection and identification of plant diseases in barley, sugar beet, and wheat among others [<xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR20">20</xref>]. Roscher et al. [<xref ref-type="bibr" rid="CR21">21</xref>] studied hyperspectral 3D plant models for detection of Cercospora leaf spot disease in sugar beet leaves. Thomas et al. [<xref ref-type="bibr" rid="CR20">20</xref>] explored <italic>Blumeria graminis</italic> f. sp <italic>hordei</italic> infection in barley using hyperspectral reflection and transmission measurements. Zhu et al. [<xref ref-type="bibr" rid="CR22">22</xref>] studied early detection of tobacco mosaic virus in plant leaves using hyperspectral imaging. Knauer et al. [<xref ref-type="bibr" rid="CR23">23</xref>] utilized hyperspectral images for improving classification accuracy of powdery mildew infection levels in wine grapes. Pandey et al. [<xref ref-type="bibr" rid="CR24">24</xref>] used hyperspectral imaging to study the chemical properties of plant leaves. Feng et al. [<xref ref-type="bibr" rid="CR25">25</xref>] determined plant water status of wheat affected by powdery mildew stress using canopy vegetation indices derived from hyperspectral data. Yeh et al. [<xref ref-type="bibr" rid="CR26">26</xref>] compared different machine learning methods for plant disease identification using hyperspectral imaging. These prior activities suggest the utility of using hyperspectral information to identify various plant diseases. Furthermore, the large data dimensions and redundancy of hyperspectral data makes machine learning based methods well suited to converting hyperspectral data into actionable information [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>].</p><p id="Par7">Deep convolutional neural networks (DCNN) have been successfully used in diverse applications such as object recognition, speech recognition, document reading and sentiment analysis [<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR32">32</xref>]. The standard convolutional filter is tailored to extract spatial features (and correlations) in 2D and is naturally suited to RGB images. In contrast, hyperspectral images can be considered as a stack of 2D images, exhibiting correlations both in space as well as in the spectral directions. To extend DCNN’s applicability to hyperspectral images, a 3D analogue of the convolutional filter was proposed and such 3D-CNN models have been used in classification of hyperspectral images for some interesting engineering applications [<xref ref-type="bibr" rid="CR33">33</xref>–<xref ref-type="bibr" rid="CR35">35</xref>]. This is a promising approach to use for hyperspectral image based classification of plant diseases. However, a potential issue with the use of such sophisticated ‘black box’ techniques is the lack of physiological insight into why the model makes a specific classification. This lack of explainability—especially when using highly detailed hyperspectral data cubes—makes the plant science community resistant to the use of these powerful techniques. The field of explainable ML models is an area of intense research effort in the machine learning community and has resulted in the development of various approaches to interrogate the learnt model to identify meaningful cues that are used for model prediction [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]. Recently, activation maps from a DCNN were used for classification and quantification of plant stress using RGB images captured using a mobile device [<xref ref-type="bibr" rid="CR38">38</xref>].</p><p id="Par8">In this work, we build upon these advances by integrating a 3D DCNN based architecture with a model explanation and visualization approach called saliency map-based visualization [<xref ref-type="bibr" rid="CR39">39</xref>] for accurate and explainable disease identification. We develop a supervised 3D-CNN based model to learn the spectral and spatial information of hyperspectral images for classification of healthy and charcoal rot infected samples. A saliency map-based visualization method is used to identify the hyperspectral wavelengths that make significant contribution to classification accuracy. We infer the importance of the wavelengths by analyzing the magnitude of saliency map gradient distribution of the image across the hyperspectral wavelengths. To the best of our knowledge, this is the first work to interpret the learnt classification model of hyperspectral data using saliency maps. This work is a societally relevant example of utilizing saliency maps to enable explanation of cues from hyperspectral data for disease identification. The availability of physiologically meaningful explanations from the saliency visualization makes us more confident in the predictions of the model.</p></sec><sec id="Sec2"><title>Materials and method</title><sec id="Sec3"><title>Plant cultivation</title><p id="Par9">Four soybean genotypes were selected for this work including Pharaoh (susceptible), DT97-4290 (moderately resistant), PI479719 (susceptible), and PI189958 (moderately resistant). The experiment was planted in four replications. Two treatments were imposed: inoculation and mock-inoculation. Each replication contained eight separate plants for each time point of data collection due to the destructive nature of lesion length measurement. Four of these plants were designated for mock-inoculation and the second set of four for inoculation. Replication 1 was planted in September 2016 and contained 5-time points of data collection post inoculation at 3, 6, 9, 12, and 15 days after inoculation (DAI). To focus on early disease detection, replications 2–4 contained 3-time points of data collection at 3, 6, and 9 DAI and were planted in November 2016. Replication 1 contained eight plants per time point (four inoculated and four mock-inoculated) for a total of 40 plants. Replications 2–4 contained eight plants per time point for a total of 72 plants. All replications were planted in growth chambers set at 30 °C with a 16-h photoperiod and were randomized within the replication. Seeds were double planted into 8 oz styrofoam cups in the growth chamber, supplemented with 0.65 g of osmocote 15-9-12, and thinned down to one plant per cup selecting the most vigorous plant 10 days after planting.</p></sec><sec id="Sec4"><title>Pathogen</title><p id="Par10">A culture of <italic>M. phaseolina</italic> (<italic>M. phaseolina</italic> 2013X), originally collected in Iowa, was used in inoculations of soybean stems. Inoculations were performed following the cut-stem inoculation method outlined in [<xref ref-type="bibr" rid="CR15">15</xref>]. Briefly, a culture of <italic>M. phaseolina</italic> cultured on potato dextrose agar (PDA) was incubated at 30° for 4 days prior to inoculations. Twenty-one days after planting, sterile 200 µL pipette tips were pushed into the media wide end down around the outer border of the culture. Soybean stems were cut, using a razor blade, 40 mm above the unifoliate node. A pipette tip, containing a plug of media + <italic>M. phaseolina</italic> for inoculated plants or PDA media alone for mock-inoculated plants was placed onto the open wound site, imbedding the tip of the stem into the media allowing the pathogen to spread into the plant tissue.</p><p id="Par11">After mock-inoculation, the mock-inoculated plants remained green and healthy. However, in response to the fungal colonization in the inoculated plants, a reddish-brown exterior lesion developed, followed by progressing dead tissue often containing black microsclerotia. A reddish-brown interior lesion also developed, often progressing farther down the inside of the stem than was visible on the exterior of the stem. To capture symptom progression, three lesion length measurements were obtained in millimeters by measuring the distance from the unifoliate node to the farthest progressed visible edge of the lesion on the exterior of the stem to prevent necrosis of the inoculated site from interfering with accurate measurements. The progression of dead tissue was measured in the same manner. Then the stems were sliced open lengthwise and the interior lesion measured in relation to the unifoliate node. Stem segments from inoculated and mock-inoculated plants were sterilized in ethanol and bleach and re-cultured onto half strength PDA media amended with chloramphenicol to inhibit bacterial growth. <italic>M. phaseolina</italic> colonies grew from the inoculated stems while no fungal colonies developed from the mock-inoculated stem culture plates fulfilling Koch’s postulates.</p></sec><sec id="Sec5"><title>Hyperspectral imaging</title><p id="Par12">Healthy and infected soybean stem samples were collected at 3, 6, 9, 12, and 15 days after charcoal rot infection. Hyperspectral data cubes of the exterior of the inoculated and mock-inoculated stems were captured at each time point of data collection prior to lesion length measurements. The imaging systems consisted of a Pika XC hyperspectral line imaging scanner, including the imager mounted on a stand, a translational stage, a laptop with Spectronon-Pro software for operating the imager and translational stage during image collection (Resonon, Bozeman, MT), and two 70-watt quartz-tungsten-halogen Illuminator lamps (ASD Inc., Boulder, CO) to provide stable illumination over a 350–2500 nm range. The Pika XC Imager collects 240 wavebands over a spectral range of 400–1000 nm with a 2.5 nm spectral resolution. The lights were positioned at a 45° angle, 54 cm away from the stem sample resting on the translational stage. The camera’s objective lens was set at an aperture of ƒ/1.4. Focus was manually adjusted in relation to the height of the camera to the stem being imaged. Exposure was automatically adjusted by the computer in response the lighting environment. The aspect ratio was set manually by adjusting frame rate and stage speed by referencing the calibration sheet and guidelines provided by Resonon. White and dark references were captured prior to imaging. The leaves were carefully removed from each soybean stem and the stems were cut at the soil line and placed one at a time on the stage for imaging. The images were captured using the Spectronon-Pro software and the hyperspectral data cubes and accompanying RGB images were saved onto an external hard drive. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the hyperspectral dataset generation procedure used in our study.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>Illustration of the hyperspectral data generation procedure used in our study</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig1_HTML.png" id="MO1"/></p></fig></p><p id="Par13">The data-set contains 111 hyperspectral stem images of size 500 × 1600 × 240 (height × length × spectral frequency). Among the 111 images, 64 represent healthy stems and 47 represent infected stems.</p></sec><sec id="Sec6"><title>Dataset pre-processing</title><p id="Par14">We used the RGB wavebands of the hyperspectral image for segmenting the charcoal rot stem in the hyperspectral image. The RGB images were transformed to HSV (Hue, Saturation and Value) color space, followed by segmenting of the charcoal rot stem by simple thresholding. Since the number of images were insufficient for training a deep learning model, we augmented the sample size of the dataset by extracting data patches of resolution 500 × 64 × 240 pixels from the 500 × 1600 × 240 resolution segmented hyperspectral images. The non-zero pixel locations in the 500 × 64 × 240 images patches were resized into 64 × 64 × 240 image patches without affecting the third dimension and were applied as input to the 3D-CNN model. The choice of the patch size resulted in enough data samples for training a 3D CNN, while ensuring that each patch contains physiologically meaningful information. The training dataset consists of 1090 images. Out of 1090 training images, 940 images represent healthy stem and 150 images represent infected stem. Although the training dataset is highly imbalanced, we were able to handle this problem in this study (see “<xref rid="Sec8" ref-type="sec">Model architecture</xref>” section). All the images were normalized between 0 and 1. The validation and test dataset consist of 194 and 539 samples, respectively. Figure <xref rid="Fig2" ref-type="fig">2</xref>a shows an example of soybean stem captured at different hyperspectral wavelengths and Fig. <xref rid="Fig2" ref-type="fig">2</xref>b shows the RGB image of the disease progression comparison between interior and exterior region of a soybean stem.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p><bold>a</bold> An example of a soybean stem imaged at different hyperspectral wavelengths. <bold>b</bold> RGB image of the disease progression comparison between interior and exterior region of soybean stem. Soybean stem was sliced in half, interior lesion length and exterior lesion length were measured in mm</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig2_HTML.png" id="MO2"/></p></fig></p></sec><sec id="Sec7"><title>Spectral reflectance</title><p id="Par15">Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates the difference in reflectance spectral between healthy and infected pixels in the charcoal rot stem. It is seen that the maximum reflectance value of infected pixels is less than the healthy pixels. We observe that the reflectance value at several wavebands decrease as the severity of the charcoal rot disease increases. We also noticed that the hyperspectral measurements near 300 nm and 1000 nm were noisy and not useful for classification.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>Illustration of reflectance spectra of healthy and infected pixels in charcoal rot stem</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig3_HTML.png" id="MO3"/></p></fig></p></sec><sec id="Sec8"><title>Model architecture</title><p id="Par16">3D-CNN models can be used to extract features jointly across the spatial and spectral dimension for classification of a 3D hyperspectral data. This is particularly useful when information (i.e. the disease signatures) are localized both in spatial and spectral domains thus exhibiting correlations in space and spectral domains. Having a model that can jointly extract features will enable accurate capture of this localized signature. The 3D-CNN model consists of two convolutional layers interspersed with two max pooling layers followed by two fully connected layers. A relatively small architecture was used to prevent overfitting during training. Two kernels of size 3 × 3 × 16 (3 × 3 in spatial dimension and 16 in spectral dimension) were used for convolving the input of the first convolution layer and four kernels of size 3 × 3 × 16 were used in the second convolution layer. Rectified Linear Input (ReLU) was used as the activation function for the convolution output [<xref ref-type="bibr" rid="CR40">40</xref>]. A 2 × 2 × 2 max pooling was applied on the output of each convolutional layer. Dropout with a probability of 0.25 was performed after first max pooling operation and with a probability of 0.5 after the first fully-connected layer. Dropout mechanism was used to prevent overfitting during training [<xref ref-type="bibr" rid="CR29">29</xref>]. The first fully-connected layer consists of 16 nodes. The output of the second fully-connected layer (2 nodes) is fed to a softmax layer. Figure <xref rid="Fig4" ref-type="fig">4</xref> summarizes the 3D convolutional neural network architecture used in the study.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>3D convolutional neural network architecture for charcoal rot image classification</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig4_HTML.png" id="MO4"/></p></fig></p></sec><sec id="Sec9"><title>Training</title><p id="Par17">The Adam optimizer was used to train our convolutional network weights based on mini-batches of size 32 [<xref ref-type="bibr" rid="CR41">41</xref>]. We used a learning rate of 10<sup>−6</sup> and set <inline-formula id="IEq1"><alternatives><mml:math><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta_{1} = 0.9$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq1.gif"/></alternatives></inline-formula>, <inline-formula id="IEq2"><alternatives><mml:math><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta_{2}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq2.gif"/></alternatives></inline-formula> = 0.999 and <inline-formula id="IEq3"><alternatives><mml:math><mml:mi>ϵ</mml:mi></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq3.gif"/></alternatives></inline-formula> = 10<sup>−8</sup>. The convolution layer kernels were initialized with normal distribution with standard deviation of 0.05. The dense layer neurons were initialized using glorot initialization [<xref ref-type="bibr" rid="CR42">42</xref>]. The 3D-CNN model was trained for 126 epochs. Here, we used all the 240 wavelength bands of hyperspectral images for classification purpose. We trained 3DCNN model using Keras [<xref ref-type="bibr" rid="CR43">43</xref>] with the Tensorflow [<xref ref-type="bibr" rid="CR44">44</xref>] backend on a NVIDIA Tesla P40 GPU. The time required for training was approximately 50 s/epoch. The plot of model accuracy on training and validation datasets during training is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Plot of model classification accuracy on training and validation data</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig5_HTML.png" id="MO5"/></p></fig></p></sec><sec id="Sec10"><title>Class balanced loss function</title><p id="Par18">Because of imbalanced training data, weighted binary cross-entropy was used as a loss function. The loss ratio was 1:6.26 between the more frequent healthy class samples and less frequent infected class samples. The class balanced loss significantly improved our classification accuracy and F1-score.</p></sec><sec id="Sec11"><title>Identification of most sensitive hyperspectral wavelengths and pixels using saliency maps</title><p id="Par19">We visualize the parts of the image that were most sensitive to the classification using an approach called class saliency map [<xref ref-type="bibr" rid="CR39">39</xref>]. Specifically, the magnitude of the gradient of the maximum predicted class score with respect to the input image was used to identify the most sensitive pixel locations for classification. While saliency maps have traditionally been used to identify spatially important pixels, we extended the notion of saliency maps to visualize the most important spectral bands used for classification. This was done as follows: Denote the set of N test hyperspectral images as <inline-formula id="IEq4"><alternatives><mml:math><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{1} ,I_{2} , \ldots I_{N }$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq4.gif"/></alternatives></inline-formula>. <italic>W</italic> is the gradient of the maximum predicted class score <inline-formula id="IEq5"><alternatives><mml:math><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{c}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq5.gif"/></alternatives></inline-formula> with respect to the input image <inline-formula id="IEq6"><alternatives><mml:math><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq6.gif"/></alternatives></inline-formula>.<disp-formula id="Equ1"><label>1</label><alternatives><mml:math display="block"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W = \frac{{\partial S_{c} }}{{\partial I_{i} }}$$\end{document}</tex-math><graphic position="anchor" xlink:href="13007_2019_479_Article_Equ1.gif"/></alternatives></disp-formula></p><p id="Par20">The magnitude of gradient quantifies how much change in each input value would change the maximum predicted class score <inline-formula id="IEq7"><alternatives><mml:math><mml:msub><mml:mi>S</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_{c}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq7.gif"/></alternatives></inline-formula>. Each pixel (<italic>x</italic>,<italic>y</italic>) in the image <inline-formula id="IEq8"><alternatives><mml:math><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq8.gif"/></alternatives></inline-formula> is maximally activated by one of the 240 wavelength channels. We denote the element index of <italic>W</italic> corresponding to a pixel location (<italic>x</italic>,<italic>y</italic>) in wavelength channel <italic>C</italic> of an image <inline-formula id="IEq9"><alternatives><mml:math><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq9.gif"/></alternatives></inline-formula> as (<inline-formula id="IEq10"><alternatives><mml:math><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:math><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x,y,C$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq10.gif"/></alternatives></inline-formula>). For each pixel location (<italic>x,y</italic>) in image <inline-formula id="IEq11"><alternatives><mml:math><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq11.gif"/></alternatives></inline-formula>, we identify the wavelength <inline-formula id="IEq12"><alternatives><mml:math><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq12.gif"/></alternatives></inline-formula> which exhibits the maximum magnitude of <italic>W</italic> across all channels. This is the most ‘salient’ wavelength. Note, that <inline-formula id="IEq13"><alternatives><mml:math><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq13.gif"/></alternatives></inline-formula> is a function of (<italic>x</italic>,<italic>y</italic>). <disp-formula id="Equ2"><label>2</label><alternatives><mml:math display="block"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext>argmax</mml:mtext><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mn>240</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:munder><mml:mfenced close="|" open="|" separators=""><mml:msub><mml:mi>W</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtext>x</mml:mtext><mml:mo>,</mml:mo><mml:mtext>y</mml:mtext><mml:mo>,</mml:mo><mml:mtext>C</mml:mtext></mml:mrow></mml:mfenced></mml:msub></mml:mfenced><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="bold-italic">for</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">all</mml:mi></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*} = \mathop {\text{argmax}}\limits_{{C \in \left( {1,2, \ldots 240} \right)}} \left| {W_{{\left( {{\text{x}},{\text{y}},{\text{C}}} \right)}} } \right| \quad \varvec{for }\;\varvec{all }\left( {x,y} \right) \in I_{i}$$\end{document}</tex-math><graphic position="anchor" xlink:href="13007_2019_479_Article_Equ2.gif"/></alternatives></disp-formula></p><p id="Par21">Another way to interpret the relative sensitivity of each hyperspectral wavelength in the learnt classifier is by summing the magnitude of all saliency gradients (L1-norm) in each wavelength. The L1-norm of the saliency gradients of a wavelength indicates the sensitivity of that wavelength in classification. Denote as <inline-formula id="IEq14"><alternatives><mml:math><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq14.gif"/></alternatives></inline-formula> (<inline-formula id="IEq15"><alternatives><mml:math><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i \in \left( {1, 2, \ldots N } \right)$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq15.gif"/></alternatives></inline-formula>) the 240-length vector containing the L1-norm of saliency gradients in each wavelength for a test image <inline-formula id="IEq16"><alternatives><mml:math><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq16.gif"/></alternatives></inline-formula> as shown in Eq. <xref rid="Equ3" ref-type="disp-formula">3</xref>. <disp-formula id="Equ3"><label>3</label><alternatives><mml:math display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:munder><mml:mfenced close="|" open="|" separators=""><mml:msub><mml:mi mathvariant="bold">W</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">C</mml:mi></mml:mrow></mml:mfenced></mml:msub></mml:mfenced><mml:mspace width="1em"/><mml:mrow><mml:mi mathvariant="bold-italic">For</mml:mi></mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mrow/><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{G}_{\varvec{i}} = \mathop \sum \limits_{\varvec{x}} \mathop \sum \limits_{\varvec{y}} \left| {{\mathbf{W}}_{{\left( {{\mathbf{x}},{\mathbf{y}},{\mathbf{C}}} \right)}} } \right| \quad \varvec{For }\left( {\varvec{x},\varvec{y}} \right)\varvec{ } \in \varvec{I}_{\varvec{i}}$$\end{document}</tex-math><graphic position="anchor" xlink:href="13007_2019_479_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par22">We consider the histogram constructed by aggregating this 240-length vector across healthy and infected images. These histograms, GH and GI (for healthy and infected, respectively), are constructed from <inline-formula id="IEq17"><alternatives><mml:math><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq17_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G_{i}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq17.gif"/></alternatives></inline-formula> as:<disp-formula id="Equ4"><label>4</label><alternatives><mml:math display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">GH</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>∈</mml:mo><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">Healthy</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">images</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mfenced close="|" open="|"><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>∈</mml:mo><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">Healthy</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">images</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mfenced><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{GH} = \varvec{ }\frac{{\mathop \sum \nolimits_{{\varvec{i}\; \in \;\varvec{Healthy }\;\varvec{images}}} \varvec{G}_{\varvec{i}} }}{{\left| {\left| {\mathop \sum \nolimits_{{\varvec{i}\; \in \;\varvec{Healthy }\;\varvec{images}}} \varvec{G}_{\varvec{i}} } \right|} \right|_{1} }}$$\end{document}</tex-math><graphic position="anchor" xlink:href="13007_2019_479_Article_Equ4.gif"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><mml:math display="block"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">GI</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>∈</mml:mo><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">Infected</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">images</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mfenced close="|" open="|"><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mo>∈</mml:mo><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">Infected</mml:mi></mml:mrow><mml:mspace width="0.277778em"/><mml:mrow><mml:mi mathvariant="bold-italic">images</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">G</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mfenced><mml:mn>1</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varvec{GI} = \varvec{ }\frac{{\mathop \sum \nolimits_{{\varvec{i}\; \in \;\varvec{Infected }\;\varvec{images}}} \varvec{G}_{\varvec{i}} }}{{\left| {\left| {\mathop \sum \nolimits_{{\varvec{i}\; \in \;\varvec{Infected}\;\varvec{ images}}} \varvec{G}_{\varvec{i}} } \right|} \right|_{1} }}$$\end{document}</tex-math><graphic position="anchor" xlink:href="13007_2019_479_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par23">This histogram can also be used to highlight the wavelengths that are most used by the classifier in making its decisions.</p></sec></sec><sec id="Sec12" sec-type="results"><title>Results</title><sec id="Sec13"><title>Classification results</title><p id="Par24">We evaluate the learnt 3D-CNN model on 539 test images. The model achieved a classification accuracy of 95.73%. The recall, precision and F1-score values of the model were 0.92, 0.82 and 0.87 respectively. The classification accuracy of 95.73% and recall value of 0.92 indicates a good generalizing capacity of the model for different stages of the disease. The F1-score of infected class of the test data was 0.87. Table <xref rid="Tab1" ref-type="table">1</xref> shows the confusion matrix of the results.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Confusion matrix</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"><p>Infected (true)</p></th><th align="left"><p>Healthy (true)</p></th></tr></thead><tbody><tr><td align="left"><p>Infected (predicted)</p></td><td char="." align="char"><p>78</p></td><td char="." align="char"><p>17</p></td></tr><tr><td align="left"><p>Healthy (predicted)</p></td><td char="." align="char"><p>6</p></td><td char="." align="char"><p>438</p></td></tr></tbody></table></table-wrap></p><p id="Par25">To understand the generalization of the model (as well as to test the robustness), we performed fivefold cross-validation. We randomly split the data into train, validation, and test subsets (60%, 20%, 20%) five times. Each of this split data is used to train a 3D-CNN. The classification accuracies of the five different models were 94.23%, 97.25%, 96.97%, 92.58% and 96.42%. The mean classification accuracy across the five models is 95.49% with a standard deviation of 2.01%.</p></sec><sec id="Sec14"><title>Saliency map visualization: identifying spatial pixels</title><p id="Par26">The saliency map visualizations of the healthy and infected samples are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. The magnitude of gradient of each pixel indicates the relative importance of the pixel in the prediction of the output class score. It is clearly seen that the saliency maps of the infected stem images have high magnitude of gradient values in the locations corresponding to the severely infected regions (reddish-brown). This indicates that the severely infected regions of the image contain the most sensitive pixels for prediction of the infected class score. For both the healthy and infected images, the saliency map gradients were concentrated around the middle region of the stem.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p>Image specific class saliency maps for the charcoal rot infected (top) and healthy (bottom) test images. The magnitude of the gradient of the maximum predicted class score with respect to the input image in the visualizations illustrates the sensitivity of the pixels to classification</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig6_HTML.png" id="MO11"/></p></fig></p></sec><sec id="Sec15"><title>Most sensitive hyperspectral wavelengths for classification using saliency maps</title><p id="Par27">The histogram of <inline-formula id="IEq18"><alternatives><mml:math><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="IEq18_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq18.gif"/></alternatives></inline-formula> from all pixel locations of the test images is shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. It illustrates the distribution of the most sensitive wavelength across all the pixels. The histogram reveals several important aspects of our model. First, wavelengths around 733 nm (<inline-formula id="IEq19"><alternatives><mml:math><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="IEq19_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq19.gif"/></alternatives></inline-formula> = 130 of the 240 bands) from the near-infrared region were the most sensitive among all of the wavelengths. Second, the 15 wavelengths in the spectral region of 703 to 744 nm were responsible for maximum magnitude of gradient values in 33% of the pixel locations of the test image. Further, the wavelengths in the visible spectra (400–700 nm) were more sensitive for the infected samples compared to the healthy samples. The NIR bands have been shown in the literature [<xref ref-type="bibr" rid="CR45">45</xref>] to indicate vegetative health and the fact that the model is picking up on a physiologically meaningful metric for classification provides more confidence in the model predictions. We also note that this hyperspectral range was identified as the most discriminative in a previous band-selection problem [<xref ref-type="bibr" rid="CR46">46</xref>].<fig id="Fig7"><label>Fig. 7</label><caption xml:lang="en"><p>Histogram of <inline-formula id="IEq22"><alternatives><mml:math><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow/><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:math><tex-math id="IEq22_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{*}$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq22.gif"/></alternatives></inline-formula> from all the test images. It illustrates the percentage of pixel locations from all N test images with maximum magnitude of saliency gradient from each wavelength for healthy and infected test images</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig7_HTML.png" id="MO12"/></p></fig></p><p id="Par28">The histograms of GH and GI (as discussed in “<xref rid="Sec9" ref-type="sec">Training</xref>” section) is shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The histograms indicate that 10 wavelengths in the region of 709 to 739 nm (wavelength numbers 120 to 132) with large <inline-formula id="IEq20"><alternatives><mml:math><mml:mrow><mml:mi mathvariant="italic">GH</mml:mi></mml:mrow></mml:math><tex-math id="IEq20_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$GH$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq20.gif"/></alternatives></inline-formula> and <inline-formula id="IEq21"><alternatives><mml:math><mml:mrow><mml:mi mathvariant="italic">GI</mml:mi></mml:mrow></mml:math><tex-math id="IEq21_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$GI$$\end{document}</tex-math><inline-graphic xlink:href="13007_2019_479_Article_IEq21.gif"/></alternatives></inline-formula> values are the most sensitive bands for classification of healthy and infected images. This again suggests that the model is utilizing physiologically meaningful wavelengths for model predictions.<fig id="Fig8"><label>Fig. 8</label><caption xml:lang="en"><p>Histogram of normalized L1-norm of saliency gradients in each wavelength for healthy (<italic>GH</italic>) and infected images (<italic>GI</italic>)</p></caption><p><graphic specific-use="HTML" mime-subtype="PNG" xlink:href="MediaObjects/13007_2019_479_Fig8_HTML.png" id="MO13"/></p></fig></p></sec></sec><sec id="Sec16" sec-type="discussion"><title>Discussion</title><p id="Par29">We used a 3D CNN model for charcoal rot disease classification because of its ability to learn the spatio-temporal features automatically without handcrafting and its ability to achieve high classification accuracy. Using a 3D CNN allows accounting for both spatial and spectral correlations simultaneously. We incorporated saliency map enabled interpretability to track the physiological insights of model predictions. Hence, we are more confident of the predictive capability of our model and its biological basis. We envision that these explainability based strategies for machine learning will be widely used in the plant science community as they decrease much of the mystery behind many current black box techniques.</p><p id="Par30">Selection of individual wavebands for the detection of disease symptoms, among other traits, is of increasing importance. Many fields in the plant sciences are expanding to be able to utilize high throughput technologies in data collection. However, utilizing the high dimensional 3D data sets takes enormous computing power promoting a need for a selection method to discriminate the most important information. In the future, trait specific band selection based on robust interpretability mechanisms will be helpful in dimensionality reduction of the large hyperspectral data and in designing a multispectral camera system for high throughput phenotyping in field conditions for an array of stress related signatures. A multispectral camera would incorporate only the most important wavelengths for a targeted set of stresses streamlining data collection and analysis necessary for monitoring and improving crop health. The approach presented in this research allows for applications in precision and high throughput phenotyping as well as precision agriculture. This approach can help increase the throughput of disease assessment, after model development in other stem diseases, enabling more robust large scale genetic studies [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR48">48</xref>].</p><p id="Par31">One potential limitation is the smaller dataset size used in this study. We perform fivefold cross-validation based assessment to test the robustness of the model. The architecture of the convolutional neural network can have a strong prior on the feature importance estimation [<xref ref-type="bibr" rid="CR49">49</xref>], and this could be more problematic in noisy saliency maps. This is an open problem in the machine learning community with several interesting avenues being currently explored.</p></sec><sec id="Sec17" sec-type="conclusions"><title>Conclusion</title><p id="Par32">We have demonstrated that a 3D CNN model can be used effectively to learn from hyperspectral data to identify charcoal rot disease in soybean stems. We have shown that saliency map visualization can be used to explain the importance of specific hyperspectral wavelengths in classification of diseased and healthy soybean stem tissue.</p></sec></body><back><ack><title>Acknowledgements</title><p>We thank Jae Brungardt, Brian Scott, and Hsiang Sing Naik for support during experimentation.</p></ack><sec sec-type="author-contribution"><title>Authors’ contributions</title><p>BG, AS, SS and AKS formulated research problem and designed approaches. SJ, AS and AKS performed experiments and collected data. KN, SJ, BG and SS developed processing workflow and performed data analytics. All authors contributed to the writing and development of the manuscript. All authors read and approved the final manuscript.</p></sec><sec><title>Funding</title><p>This work was funded by Iowa Soybean Association (AS), NSF/USDA NIFA Grant 2017-67007-26151 (SS, BG, AS, AKS), ISU Research Grant through the PIIR award (AS, AKS, SS, BG), Monsanto Chair in Soybean Breeding at Iowa State University (AKS), R F Baker Center for Plant Breeding (AKS), PSI Faculty Fellow award (BG, SS, AKS), and USDA-CRIS IOW04314 project (AS, AKS).</p></sec><sec sec-type="data-availability"><title>Availability of data and materials</title><p>The datasets used and/or analyzed during the current study available from the corresponding author on reasonable request.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1"><title>Ethics approval and consent to participate</title><p id="Par33">Not applicable.</p></sec><sec id="FPar2"><title>Consent for publication</title><p id="Par34">Not applicable.</p></sec><sec id="FPar3" sec-type="COI-statement"><title>Competing interests</title><p id="Par35">The authors declare that they have no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">USDA NASS. Acreage. 2016. <ext-link xlink:href="http://www.usda.gov/nass/PUBS/TODAYRPT/acrg0616.pdf" ext-link-type="url">http://www.usda.gov/nass/PUBS/TODAYRPT/acrg0616.pdf</ext-link>. Accessed 24 Apr 2018.</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hartman</surname><given-names>GL</given-names></name><name><surname>Rupe</surname><given-names>JC</given-names></name><name><surname>Sikora</surname><given-names>EF</given-names></name><name><surname>Domier</surname><given-names>LL</given-names></name><name><surname>Davis</surname><given-names>JA</given-names></name><name><surname>Steffey</surname><given-names>KL</given-names></name></person-group><source>Compendium of soybean diseases and pests</source><year>2015</year><publisher-loc>St. Paul</publisher-loc><publisher-name>The American Phytopathological Society</publisher-name></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>TW</given-names></name><name><surname>Bradley</surname><given-names>CA</given-names></name><name><surname>Sisson</surname><given-names>AJ</given-names></name><name><surname>Byamukama</surname><given-names>E</given-names></name><name><surname>Chilvers</surname><given-names>MI</given-names></name><name><surname>Coker</surname><given-names>CM</given-names></name><etal/></person-group><article-title xml:lang="en">Soybean yield loss estimates due to diseases in the United States and Ontario, Canada, from 2010 to 2014</article-title><source>Plant Health Prog</source><year>2017</year><volume>18</volume><fpage>19</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.1094/PHP-RS-16-0066</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akintayo</surname><given-names>A</given-names></name><name><surname>Tylka</surname><given-names>GL</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">A deep learning framework to discern and count microscopic nematode eggs</article-title><source>Sci Rep</source><year>2018</year><volume>8</volume><fpage>9145</fpage><pub-id pub-id-type="doi">10.1038/s41598-018-27272-w</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>CH</given-names></name><name><surname>Poole</surname><given-names>GH</given-names></name><name><surname>Parker</surname><given-names>PE</given-names></name><name><surname>Gottwald</surname><given-names>TR</given-names></name></person-group><article-title xml:lang="en">Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging</article-title><source>Crit Rev Plant Sci</source><year>2010</year><volume>29</volume><fpage>59</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1080/07352681003617285</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naik</surname><given-names>HS</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Lofquist</surname><given-names>A</given-names></name><name><surname>Assefa</surname><given-names>T</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name><name><surname>Ackerman</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">A real-time phenotyping framework using machine learning for plant stress severity rating in soybean</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><fpage>23</fpage><pub-id pub-id-type="doi">10.1186/s13007-017-0173-7</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Naik</surname><given-names>HS</given-names></name><name><surname>Assefa</surname><given-names>T</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name><name><surname>Reddy</surname><given-names>RVC</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Computer vision and machine learning for robust phenotyping in genome-wide studies</article-title><source>Sci Rep</source><year>2017</year><volume>7</volume><fpage>44048</fpage><pub-id pub-id-type="doi">10.1038/srep44048</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Koenning SR, Wrather JA. Suppression of soybean yield potential in the continental United States by plant diseases from 2006 to 2009. Plant Health Prog. 2010. <ext-link xlink:href="http://www.plantmanagementnetwork.org/pub/php/research/2010/yield/" ext-link-type="url">http://www.plantmanagementnetwork.org/pub/php/research/2010/yield/</ext-link>. Accessed 24 Apr 2018.</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Short</surname><given-names>GE</given-names></name></person-group><article-title xml:lang="en">Survival of <italic>Macrophomina phaseolina</italic> in soil and in residue of soybean</article-title><source>Phytopathology</source><year>1980</year><volume>70</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.1094/Phyto-70-13</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Su</surname><given-names>G</given-names></name><name><surname>Suh</surname><given-names>SO</given-names></name><name><surname>Schneider</surname><given-names>RW</given-names></name><name><surname>Russin</surname><given-names>JS</given-names></name></person-group><article-title xml:lang="en">Host specialization in the charcoal rot fungus, <italic>Macrophomina phaseolina</italic></article-title><source>Phytopathology</source><year>2001</year><volume>91</volume><fpage>120</fpage><pub-id pub-id-type="coi">1:STN:280:DC%2BD1cjjslOhug%3D%3D</pub-id><pub-id pub-id-type="doi">10.1094/PHYTO.2001.91.2.120</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romero Luna</surname><given-names>MP</given-names></name><name><surname>Mueller</surname><given-names>D</given-names></name><name><surname>Mengistu</surname><given-names>A</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Hartman</surname><given-names>GL</given-names></name><name><surname>Wise</surname><given-names>KA</given-names></name></person-group><article-title xml:lang="en">Advancing our understanding of charcoal rot in soybeans</article-title><source>J Integr Pest Manag</source><year>2017</year><volume>8</volume><fpage>8</fpage><pub-id pub-id-type="doi">10.1093/jipm/pmw020</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>GKG</given-names></name><name><surname>Sharma</surname><given-names>SSK</given-names></name><name><surname>Ramteke</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Biology, epidemiology and management of the pathogenic fungus <italic>Macrophomina phaseolina</italic> (Tassi) Goid with special reference to charcoal rot of soybean (Glycine max (L.) Merrill)</article-title><source>J Phytopathol</source><year>2012</year><volume>160</volume><fpage>167</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1111/j.1439-0434.2012.01884.x</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pawlowski</surname><given-names>ML</given-names></name><name><surname>Hill</surname><given-names>CB</given-names></name><name><surname>Hartman</surname><given-names>GL</given-names></name></person-group><article-title xml:lang="en">Resistance to charcoal rot identified in ancestral soybean germplasm</article-title><source>Crop Sci</source><year>2015</year><volume>55</volume><fpage>1230</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.2135/cropsci2014.10.0687</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mengistu</surname><given-names>A</given-names></name><name><surname>Ray</surname><given-names>JD</given-names></name><name><surname>Smith</surname><given-names>JR</given-names></name><name><surname>Paris</surname><given-names>RL</given-names></name></person-group><article-title xml:lang="en">Charcoal rot disease assessment of soybean genotypes using a colony-forming unit index</article-title><source>Crop Sci</source><year>2007</year><volume>47</volume><fpage>2453</fpage><lpage>2461</lpage><pub-id pub-id-type="doi">10.2135/cropsci2007.04.0186</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twizeyimana</surname><given-names>M</given-names></name><name><surname>Hill</surname><given-names>CB</given-names></name><name><surname>Pawlowski</surname><given-names>M</given-names></name><name><surname>Paul</surname><given-names>C</given-names></name><name><surname>Hartman</surname><given-names>GL</given-names></name></person-group><article-title xml:lang="en">A cut-stem inoculation technique to evaluate soybean for resistance to <italic>Macrophomina phaseolina</italic></article-title><source>Plant Dis</source><year>2012</year><volume>96</volume><fpage>1210</fpage><lpage>1215</lpage><pub-id pub-id-type="coi">1:STN:280:DC%2BB3cjpslKmsg%3D%3D</pub-id><pub-id pub-id-type="doi">10.1094/PDIS-02-12-0126-RE</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coser</surname><given-names>SM</given-names></name><name><surname>Chowda Reddy</surname><given-names>RV</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Mueller</surname><given-names>DS</given-names></name><name><surname>Mengistu</surname><given-names>A</given-names></name><name><surname>Wise</surname><given-names>KA</given-names></name><etal/></person-group><article-title xml:lang="en">Genetic architecture of charcoal rot (<italic>Macrophomina phaseolina</italic>) resistance in soybean revealed using a diverse panel</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><fpage>1626</fpage><pub-id pub-id-type="doi">10.3389/fpls.2017.01626</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>A</given-names></name><name><surname>Harrison</surname><given-names>N</given-names></name><name><surname>French</surname><given-names>AP</given-names></name></person-group><article-title xml:lang="en">Hyperspectral image analysis techniques for the detection and classification of the early onset of plant disease and stress</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><fpage>80</fpage><pub-id pub-id-type="doi">10.1186/s13007-017-0233-z</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elke</surname><given-names>B</given-names></name><name><surname>Werner</surname><given-names>BH</given-names></name></person-group><article-title xml:lang="en">Hyperspectral and chlorophyll fluorescence imaging for early detection of plant diseases, with special reference to <italic>Fusarium</italic> spec. infections on wheat</article-title><source>Agriculture</source><year>2014</year><volume>4</volume><fpage>32</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.3390/agriculture4010032</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuska</surname><given-names>MT</given-names></name><name><surname>Brugger</surname><given-names>A</given-names></name><name><surname>Thomas</surname><given-names>S</given-names></name><name><surname>Wahabzada</surname><given-names>M</given-names></name><name><surname>Kersting</surname><given-names>K</given-names></name><name><surname>Oerke</surname><given-names>E-C</given-names></name><etal/></person-group><article-title xml:lang="en">Spectral patterns reveal early resistance reactions of barley against <italic>Blumeria graminis</italic> f. sp. hordei</article-title><source>Phytopathology</source><year>2017</year><volume>107</volume><fpage>1388</fpage><lpage>1398</lpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC1MXps1GhtA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1094/PHYTO-04-17-0128-R</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>S</given-names></name><name><surname>Wahabzada</surname><given-names>M</given-names></name><name><surname>Kuska</surname><given-names>MT</given-names></name><name><surname>Rascher</surname><given-names>U</given-names></name><name><surname>Mahlein</surname><given-names>AK</given-names></name></person-group><article-title xml:lang="en">Observation of plant-pathogen interaction by simultaneous hyperspectral imaging reflection and transmission measurements</article-title><source>Funct Plant Biol</source><year>2017</year><volume>44</volume><fpage>23</fpage><lpage>34</lpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC28XitFSmt7%2FI</pub-id><pub-id pub-id-type="doi">10.1071/FP16127</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roscher</surname><given-names>R</given-names></name><name><surname>Behmann</surname><given-names>J</given-names></name><name><surname>Mahlein</surname><given-names>A-K</given-names></name><name><surname>Dupuis</surname><given-names>J</given-names></name><name><surname>Kuhlmann</surname><given-names>H</given-names></name><name><surname>Plümer</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Detection of disease symptoms on hyperspectral 3D plant models</article-title><source>ISPRS Ann Photogramm Remote Sens Spat Inf Sci</source><year>2016</year><volume>3</volume><fpage>88</fpage><lpage>96</lpage></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Chu</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Jiang</surname><given-names>L</given-names></name><name><surname>He</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Hyperspectral imaging for presymptomatic detection of tobacco disease with successive projections algorithm and machine-learning classifiers</article-title><source>Sci Rep</source><year>2017</year><volume>7</volume><fpage>4125</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-04501-2</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knauer</surname><given-names>U</given-names></name><name><surname>Matros</surname><given-names>A</given-names></name><name><surname>Petrovic</surname><given-names>T</given-names></name><name><surname>Zanker</surname><given-names>T</given-names></name><name><surname>Scott</surname><given-names>ES</given-names></name><name><surname>Seiffert</surname><given-names>U</given-names></name></person-group><article-title xml:lang="en">Improved classification accuracy of powdery mildew infection levels of wine grapes by spatial-spectral analysis of hyperspectral images</article-title><source>Plant Methods</source><year>2017</year><volume>13</volume><fpage>47</fpage><pub-id pub-id-type="doi">10.1186/s13007-017-0198-y</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandey</surname><given-names>P</given-names></name><name><surname>Ge</surname><given-names>Y</given-names></name><name><surname>Stoerger</surname><given-names>V</given-names></name><name><surname>Schnable</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">High throughput in vivo analysis of plant leaf chemical properties using hyperspectral imaging</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><fpage>1348</fpage><pub-id pub-id-type="doi">10.3389/fpls.2017.01348</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feng</surname><given-names>W</given-names></name><name><surname>Qi</surname><given-names>S</given-names></name><name><surname>Heng</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><etal/></person-group><article-title xml:lang="en">Canopy vegetation indices from in situ hyperspectral data to assess plant water status of winter wheat under powdery mildew stress</article-title><source>Front Plant Sci</source><year>2017</year><volume>8</volume><fpage>1219</fpage><pub-id pub-id-type="doi">10.3389/fpls.2017.01219</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>Y-HF</given-names></name><name><surname>Chung</surname><given-names>W-C</given-names></name><name><surname>Liao</surname><given-names>J-Y</given-names></name><name><surname>Chung</surname><given-names>C-L</given-names></name><name><surname>Kuo</surname><given-names>Y-F</given-names></name><name><surname>Lin</surname><given-names>T-T</given-names></name></person-group><article-title xml:lang="en">A comparison of machine learning methods on hyperspectral plant disease assessments</article-title><source>IFAC Proc Vol</source><year>2013</year><volume>46</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.3182/20130327-3-JP-3017.00081</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Machine learning for high-throughput stress phenotyping in plants</article-title><source>Trends Plant Sci</source><year>2016</year><volume>21</volume><fpage>110</fpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC2MXhvVykurvJ</pub-id><pub-id pub-id-type="doi">10.1016/j.tplants.2015.10.015</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep learning for plant stress phenotyping: trends and future perspectives</article-title><source>Trends Plant Sci</source><year>2018</year><volume>23</volume><fpage>883</fpage><lpage>898</lpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC1cXhsVSmtrnM</pub-id><pub-id pub-id-type="doi">10.1016/j.tplants.2018.07.004</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in neural information processing systems. 2012. p. 1097–105.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waibel</surname><given-names>A</given-names></name><name><surname>Hanazawa</surname><given-names>T</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Shikano</surname><given-names>K</given-names></name><name><surname>Lang</surname><given-names>KJ</given-names></name></person-group><article-title xml:lang="en">Phoneme recognition using time-delay neural networks</article-title><source>IEEE Trans Acoust</source><year>1989</year><volume>37</volume><fpage>328</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1109/29.21701</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Gradient-based learning applied to document recognition</article-title><source>Proc IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Dos Santos CN, Gatti M. Deep convolutional neural networks for sentiment analysis of short texts. In: COLING. 2014. p. 69–78.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fotiadou</surname><given-names>K</given-names></name><name><surname>Tsagkatakis</surname><given-names>G</given-names></name><name><surname>Tsakalides</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Deep convolutional neural networks for the classification of snapshot mosaic hyperspectral imagery</article-title><source>Electron Imaging</source><year>2017</year><volume>2017</volume><fpage>185</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.2352/ISSN.2470-1173.2017.17.COIMG-445</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Jiang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Member</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</article-title><source>IEE Trans Geosci Remote Sens</source><year>2016</year><volume>54</volume><fpage>6232</fpage><lpage>6251</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2016.2584107</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Shen</surname><given-names>Q</given-names></name></person-group><article-title xml:lang="en">Spectral-spatial classification of hyperspectral imagery with 3D convolutional neural network</article-title><source>Remote Sens</source><year>2017</year><volume>9</volume><fpage>67</fpage><pub-id pub-id-type="doi">10.3390/rs9010067</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montavon</surname><given-names>G</given-names></name><name><surname>Samek</surname><given-names>W</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name></person-group><article-title xml:lang="en">Methods for interpreting and understanding deep neural networks</article-title><source>Digit Signal Process</source><year>2018</year><volume>73</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.dsp.2017.10.011</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Balu A, Nguyen TV, Kokate A, Hegde C, Sarkar S. A forward-backward approach for visualizing information flow in deep networks. 2017. <ext-link xlink:href="http://arxiv.org/abs/1711.06221" ext-link-type="url">http://arxiv.org/abs/1711.06221</ext-link>.</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosal</surname><given-names>S</given-names></name><name><surname>Blystone</surname><given-names>D</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">An explainable deep machine vision framework for plant stress phenotyping</article-title><source>Proc Natl Acad Sci</source><year>2018</year><volume>115</volume><fpage>4613</fpage><lpage>4618</lpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC1cXitlWrtLvM</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1716999115</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A. Deep inside convolutional networks: visualising image classification models and saliency maps. arXiv preprint <ext-link xlink:href="http://arxiv.org/abs/13126034" ext-link-type="url">http://arxiv.org/abs/13126034</ext-link>. 2013.</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Glorot X, Bordes A, Bengio Y. Deep sparse rectifier neural networks. In: Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011. p. 315–23.</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Kingma D, Ba J. Adam: a method for stochastic optimization. arXiv preprint <ext-link xlink:href="http://arxiv.org/abs/14126980" ext-link-type="url">http://arxiv.org/abs/14126980</ext-link>. 2014.</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010. p. 249–56.</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Chollet F, others. Keras. 2015. <ext-link xlink:href="https://keras.io" ext-link-type="url">https://keras.io</ext-link>. Accessed 24 Apr 2018.</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al. TensorFlow: a system for large-scale machine learning. Osdi. 2016. p. 265–83.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knipling</surname><given-names>EB</given-names></name></person-group><article-title xml:lang="en">Physical and physiological basis for the reflectance of visible and near-infrared radiation from vegetation</article-title><source>Remote Sens Environ</source><year>1970</year><volume>1</volume><fpage>155</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/S0034-4257(70)80021-9</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagasubramanian</surname><given-names>K</given-names></name><name><surname>Jones</surname><given-names>S</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Ganapathysubramanian</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Hyperspectral band selection using genetic algorithm and support vector machines for early identification of charcoal rot disease in soybean stems</article-title><source>Plant Methods</source><year>2018</year><volume>14</volume><fpage>86</fpage><pub-id pub-id-type="doi">10.1186/s13007-018-0349-9</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moellers</surname><given-names>TC</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Brungardt</surname><given-names>J</given-names></name><name><surname>Kabbage</surname><given-names>M</given-names></name><name><surname>Mueller</surname><given-names>DS</given-names></name><etal/></person-group><article-title xml:lang="en">Main and epistatic loci studies in soybean for <italic>Sclerotinia sclerotiorum</italic> resistance reveal multiple modes of resistance in multi-environments</article-title><source>Sci Rep</source><year>2017</year><volume>7</volume><fpage>3554</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-03695-9</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name><name><surname>Mueller</surname><given-names>DS</given-names></name><name><surname>Singh</surname><given-names>AK</given-names></name></person-group><article-title xml:lang="en">Genome-wide association and epistasis studies unravel the genetic architecture of sudden death syndrome resistance in soybean</article-title><source>Plant J</source><year>2015</year><volume>84</volume><fpage>1124</fpage><lpage>1136</lpage><pub-id pub-id-type="coi">1:CAS:528:DC%2BC2MXitVKit77F</pub-id><pub-id pub-id-type="doi">10.1111/tpj.13069</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Adebayo J, Gilmer J, Goodfellow I, Kim B. Local explanation methods for deep neural networks lack sensitivity to parameter values. arXiv Preprint <ext-link xlink:href="http://arxiv.org/abs/181003307" ext-link-type="url">http://arxiv.org/abs/181003307</ext-link>. 2018.</mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><title>Publisher's Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Biological Techniques</facet-value><facet-value count="1">Life Sciences</facet-value><facet-value count="1">Plant Sciences</facet-value></facet><facet name="keyword"><facet-value count="1">Charcoal rot disease</facet-value><facet-value count="1">Deep convolutional neural network</facet-value><facet-value count="1">Hyperspectral</facet-value><facet-value count="1">Saliency map</facet-value><facet-value count="1">Soybean</facet-value></facet><facet name="pub"><facet-value count="1">Plant Methods</facet-value></facet><facet name="year"><facet-value count="1">2019</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
